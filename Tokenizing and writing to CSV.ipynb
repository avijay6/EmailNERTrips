{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e68eb43",
   "metadata": {},
   "source": [
    "##Text cleaning###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sutime import SUTime\n",
    "\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "import neuralcoref\n",
    "from pathlib import Path\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "import json\n",
    "import nltk.data\n",
    "import os\n",
    "import spacy\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import csv\n",
    "from nltk import sent_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd867bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags_func(text):\n",
    "    '''\n",
    "    Removes HTML-Tags from a string, if present\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string without HTML-Tags\n",
    "    ''' \n",
    "    return BeautifulSoup(text, 'html.parser').get_text()\n",
    "def remove_url_func(text):\n",
    "    '''\n",
    "    Removes URL addresses from a string, if present\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string without URL addresses\n",
    "    ''' \n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "def remove_accented_chars_func(text):\n",
    "    '''\n",
    "    Removes all accented characters from a string, if present\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string without accented characters\n",
    "    '''\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "def remove_punctuation_func(text):\n",
    "    '''\n",
    "    Removes all punctuation from a string, if present\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string without punctuations\n",
    "    '''\n",
    "    return re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "def remove_irr_char_func(text):\n",
    "    '''\n",
    "    Removes all irrelevant characters (numbers and punctuation) from a string, if present\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string without irrelevant characters\n",
    "    '''\n",
    "    return re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "def remove_extra_whitespaces_func(text):\n",
    "    '''\n",
    "    Removes extra whitespaces from a string, if present\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Clean string without extra whitespaces\n",
    "    ''' \n",
    "    return re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()\n",
    "def word_count_func(text):\n",
    "    '''\n",
    "    Counts words within a string\n",
    "    \n",
    "    Args:\n",
    "        text (str): String to which the function is to be applied, string\n",
    "    \n",
    "    Returns:\n",
    "        Number of words within a string, integer\n",
    "    ''' \n",
    "    return len(text.split())\n",
    "\n",
    "def relativedate(text,absdate):\n",
    "    sutime = SUTime(mark_time_ranges=True, include_range=True)\n",
    "    \n",
    "    with open('timerelative', 'w') as fout:\n",
    "        json.dump(sutime.parse(text,absdate), sort_keys=True, indent=4)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coref_resolution(text):\n",
    "    \"\"\"Function that executes coreference resolution on a given text\"\"\"\n",
    "    doc = nlp(text)\n",
    "    # fetches tokens with whitespaces from spacy document\n",
    "    tok_list = list(token.text_with_ws for token in doc)\n",
    "    for cluster in doc._.coref_clusters:\n",
    "        # get tokens from representative cluster name\n",
    "        cluster_main_words = set(cluster.main.text.split(' '))\n",
    "        for coref in cluster:\n",
    "            if coref is not None:\n",
    "                if coref != cluster.main:  # if coreference element is not the representative element of that cluster\n",
    "                    if coref.text != cluster.main.text and bool(set(coref.text.split(' ')).intersection(cluster_main_words)) == False:\n",
    "                    # if coreference element text and representative element text are not equal and none of the coreference element words are in representative element. This was done to handle nested coreference scenarios\n",
    "                        tok_list[coref.start] = cluster.main.text + \\\n",
    "                            doc[coref.end-1].whitespace_\n",
    "                        for i in range(coref.start+1, coref.end):\n",
    "                            tok_list[i] = \"\"\n",
    "\n",
    "    return \"\".join(tok_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29798859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textcleaning(text):\n",
    "    text_html = remove_html_tags_func(text)\n",
    "    text_url = remove_url_func(text_html) \n",
    "    text_acc = remove_accented_chars_func(text_url)\n",
    "    #text_punct = remove_punctuation_func(text_acc)\n",
    "    #text_irr = remove_irr_char_func(text_acc)   \n",
    "    text_clean = remove_extra_whitespaces_func(text_acc)\n",
    "   \n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70645092",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "\n",
    "user_input = input('What is the name of your directory')\n",
    "directory = os.listdir(user_input)\n",
    "\n",
    "emailno=0\n",
    "file = open(\"filejun23.csv\", \"w\")\n",
    "#f1 = open(\"inputjun23.txt\", \"w\")\n",
    "for fname in directory:\n",
    "        if os.path.isfile(user_input + os.sep + fname):\n",
    "       \n",
    "            with open(user_input + os.sep + fname, 'rb') as f:\n",
    "                name = f.name  # Get file name\n",
    "                msg = BytesParser(policy=policy.default).parse(f)\n",
    "                absdate = msg['Date']\n",
    "            print(name)\n",
    "            \n",
    "                #preferencelist=('plain')\n",
    "            if (msg.get_body(preferencelist=('plain'))):\n",
    "                emailno=emailno+1\n",
    "                print(emailno)\n",
    "                text = msg.get_body(preferencelist=('plain')).get_content()\n",
    "            \n",
    "                text_content = coref_resolution(text)\n",
    "                \n",
    "                text_content = textcleaning(text_content)\n",
    "                punctuations = '''“'!()—-―[]{};:'\"\\<>/?@#$%^&*_~'''\n",
    "                \n",
    "                no_punct = \"\"\n",
    "                for char in text_content:\n",
    "                    if char not in punctuations:\n",
    "                        no_punct = no_punct + char\n",
    "                        \n",
    "                text_content = no_punct \n",
    "                #relativedate(text_content, absdate)\n",
    "                #f1.write(text_content)\n",
    "                #wrdcnt = word_count_func(text_content)\n",
    "                #f= open(\"processed\"+fname+\".txt\",\"w+\")\n",
    "                #f.write(text_content)\n",
    "                #f.close()\n",
    "                print(fname)\n",
    "                applyNER(emailno, fname, text_content)\n",
    "                #NER(text_content,emailno,name)\n",
    "                #ner_new(text_content,emailno,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyNER(emailno,fname,text):\n",
    "#text = (\"BILL IS TRAVELLING FROM CHICAGO TO DALLAS THIS SUNDAY.\")\n",
    "    \n",
    "    \n",
    "    text=remove_extra_whitespaces_func(text)\n",
    "    sent= text.split('.')\n",
    "    for t in sent:\n",
    "        text1=coref_resolution(t)\n",
    "    \n",
    "    \n",
    "    doc = nlp(text)\n",
    "    words = []\n",
    "    sente = []\n",
    "    #labels = []\n",
    "    sent_i =0\n",
    "    \n",
    "    for sent_i,sent in enumerate(doc.sents):\n",
    "       \n",
    "        for token in sent:\n",
    "            if token.ent_type_:\n",
    "            #print(token.text, token.ent_iob_, token.ent_type_)\n",
    "                words.append(token.text)\n",
    "                sente.append(sent_i)\n",
    "            #iob.append(token.ent_iob_)\n",
    "                #labels.append(token.ent_iob_+'-'+token.ent_type_)\n",
    "                \n",
    "                #labels.append()\n",
    "            else:\n",
    "                words.append(token.text)\n",
    "                sente.append(sent_i)\n",
    "            #iob.append()\n",
    "                #labels.append(token.ent_iob_)\n",
    "            #print(token.text, token.ent_iob_, 'NULL')\n",
    "    df = pd.DataFrame({'emailno' : fname, 'sentno': sente, 'word': words, 'label': 'O'})\n",
    "    #df.to_csv(r\"{}.csv\".format(fname), index=False) # biluo in extension to indicate the type of encoding, it is ok to keep csv\n",
    "\n",
    "    df.to_csv('filejune23.csv', mode='a', index=False, header=False)\n",
    "    #path = r\"{}.csv\".format(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input('What is the name of your directory')\n",
    "directory = os.listdir(user_input)\n",
    "\n",
    "emailno=0\n",
    "\n",
    "file = open(\"filejun23.csv\", \"w\")\n",
    "#f1 = open(\"inputjun23.txt\", \"w\")\n",
    "for fname in directory:\n",
    "        if os.path.isfile(user_input + os.sep + fname):\n",
    "       \n",
    "            with open(user_input + os.sep + fname, 'rb') as f:\n",
    "                name = f.name  # Get file name\n",
    "                msg = BytesParser(policy=policy.default).parse(f)\n",
    "                absdate = msg['Date']\n",
    "            print(name)\n",
    "            \n",
    "                #preferencelist=('plain')\n",
    "            if (msg.get_body(preferencelist=('plain'))):\n",
    "                emailno=emailno+1\n",
    "                print(emailno)\n",
    "                text = msg.get_body(preferencelist=('plain')).get_content()\n",
    "            \n",
    "                text_content = coref_resolution(text)\n",
    "                print(text_content)\n",
    "                text_content = textcleaning(text_content)\n",
    "                punctuations = '''“'!()—-―[]{};:'\"\\<>/?@#$%^&*_~'''\n",
    "                \n",
    "                no_punct = \"\"\n",
    "                for char in text_content:\n",
    "                    if char not in punctuations:\n",
    "                        no_punct = no_punct + char\n",
    "                        \n",
    "                text_content = no_punct \n",
    "                #relativedate(text_content, absdate)\n",
    "                #f1.write(text_content)\n",
    "                #wrdcnt = word_count_func(text_content)\n",
    "                #f= open(\"processed\"+fname+\".txt\",\"w+\")\n",
    "                #f.write(text_content)\n",
    "                #f.close()\n",
    "                print(fname)\n",
    "                applyNER(emailno, fname, text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a74c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
