{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe46ace2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the name of your directorytestdata\n",
      "1217.eml\n",
      "1161.eml\n",
      "1229.eml\n",
      "3943.eml\n",
      ".DS_Store\n",
      "invalid\n",
      "141.eml\n",
      "1760.eml\n",
      "1171.eml\n",
      "1005.eml\n",
      "1937.eml\n",
      "454.eml\n",
      "293.eml\n",
      "6493.eml\n",
      "1302.eml\n",
      "1921.eml\n",
      "1716.eml\n",
      "7213.eml\n",
      "1717.eml\n",
      "730.eml\n",
      "478.eml\n",
      "652.eml\n",
      "691.eml\n",
      "875.eml\n",
      "492.eml\n",
      "525.eml\n",
      "5542.eml\n",
      "1479.eml\n",
      "1482.eml\n",
      "1496.eml\n",
      "1865.eml\n",
      "1695.eml\n",
      "2015.eml\n",
      "3321.eml\n",
      "465.eml\n",
      "148.eml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#read files from the directory, rad the messages, split them into lines and write to another file\n",
    "import pandas as pd \n",
    "from pathlib import Path\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "import json\n",
    "import nltk.data\n",
    "import re\n",
    "from re import search\n",
    "import os\n",
    "import spacy\n",
    "import csv\n",
    "import neuralcoref\n",
    "keywords=['travel','tour','landed','trip','flying','visited','visit','journeyed','en route','meeting']\n",
    "user_input = input('What is the name of your directory')\n",
    "directory = os.listdir(user_input)\n",
    "#print(directory)\n",
    "eno=0;\n",
    "i=1\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "punctuations = '''!()—-―[]{};:'\"\\<>/?@#$%^&*_~'''\n",
    "\n",
    "for fname in directory:\n",
    "        lines=[]\n",
    "        eno = eno+1\n",
    "        \n",
    "        if os.path.isfile(user_input + os.sep + fname):\n",
    "            print(fname)\n",
    "            with open(user_input + os.sep + fname, 'rb') as f:\n",
    "                name = f.name  # Get file name\n",
    "                msg = BytesParser(policy=policy.default).parse(f)\n",
    "            #print_payload(msg)   \n",
    "            if(msg.get_body(preferencelist=('plain'))):\n",
    "                text = msg.get_body(preferencelist=('plain')).get_content()    \n",
    "                #print(text)\n",
    "            else:\n",
    "                print('invalid')\n",
    "                #break\n",
    "            text = textcleaning(text)\n",
    "            \n",
    "            #split to sentence and find if the keyword is in the sentence; perform NER and add this file to hitlist only if \n",
    "            #there are a person entity and a location in that sentence\n",
    "            tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "            sent= text.split('.')\n",
    "            sentid=0\n",
    "            for t in sent:\n",
    "                sentid=sentid+1\n",
    "                locn=0\n",
    "                doc=nlp(t)\n",
    "                #print(t)\n",
    "                for ent in doc.ents:\n",
    "        \n",
    "                    if ent.label_==\"LOC\" or ent.label_== \"GPE\":\n",
    "                        locn=locn+1\n",
    "                   \n",
    "                for searchstring in keywords:\n",
    "                    res = bytes(searchstring, 'utf-8')\n",
    "                    if ( (locn > 0) and (res.decode(\"utf-8\") in t)):\n",
    "                        #relevant text\n",
    "                        \n",
    "                        f2 = open('relevantemails.csv', 'a')\n",
    "                        writer = csv.writer(f2)\n",
    "                        writer.writerow([eno,sentid,t])\n",
    "                        \n",
    "                        #with open('relevant_emails.txt', 'a') as f1:\n",
    "                         #   f1.write(t)\n",
    "                          #  f1.write(\"\\n\")\n",
    "                           # break\n",
    "                            \n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92015aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "f2 = open(\"index.csv\", \"w\")\n",
    "writer = csv.writer(f2)\n",
    "with open('relevantemails.csv', \"r\", encoding=\"utf-8\") as f:\n",
    "        csvreader=csv.reader(f, delimiter=',')\n",
    "\n",
    "        for row in csvreader:\n",
    "            writer.writerow([row[0],row[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb99c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "a = pd.read_csv(\"index.csv\")\n",
    "b = pd.read_csv(\"table1.csv\")\n",
    "\n",
    "#merged = a.merge(b)\n",
    "#merged.to_csv(\"output.csv\", index=False)\n",
    "\n",
    "\n",
    "horizontal_stack = pd.concat([a,b], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8473fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textcleaning(text):\n",
    "    ###tokenising the email text\n",
    "# define punctuation\n",
    "    punctuations = '''!()—-―[]{};:'\"\\<>/?@#$%^&*_~'''\n",
    "\n",
    "\n",
    "# Replace all occurrences of character s with an empty string\n",
    "\n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text, flags=re.MULTILINE)\n",
    "    text = text.replace(\"httpgetwrap\",\" \")\n",
    "    text= text.replace(\" Sec. \", \"Secretary\")\n",
    "    text= text.replace(\" Rep. \", \"Representative\")\n",
    "    text= text.replace(\" Sen. \", \"Senator\")\n",
    "    text= text.replace(\" U.S \", \"US\")\n",
    "    text= text.replace(\" Gov. \", \"Governor\")\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "#text = text.replace(\"today\",msg['Date'])\n",
    "#print(text)\n",
    "# remove punctuation from the string\n",
    "    no_punct = \"\"\n",
    "    for char in text:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f072bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_stack.to_csv('output.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11809962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B-airline_name\n",
    "#I-airline_name\n",
    "B-arrive_date.date_relative\n",
    "B-arrive_date.day_name\n",
    "B-arrive_date.day_number\n",
    "I-arrive_date.day_number\n",
    "B-arrive_date.month_name\n",
    "B-arrive_date.today_relative\n",
    "B-depart_date.date_relative\n",
    "B-depart_date.day_name\n",
    "B-depart_date.day_number\n",
    "I-depart_date.day_number\n",
    "B-depart_date.month_name\n",
    "B-depart_date.today_relative\n",
    "I-depart_date.today_relative\n",
    "B-depart_date.year\n",
    "B-depart_time.time\n",
    "I-depart_time.time\n",
    "B-depart_time.time_relative\n",
    "I-depart_time.time_relative\n",
    "B-fromloc.city_name\n",
    "I-fromloc.city_name\n",
    "B-toloc.city_name\n",
    "I-toloc.city_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
